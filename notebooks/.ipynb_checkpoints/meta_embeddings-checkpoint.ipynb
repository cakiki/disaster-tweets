{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Word Meta-Embeddings by Autoencoding\n",
    "\n",
    "https://www.aclweb.org/anthology/C18-1140/\n",
    "\n",
    "Danushka Bollegala, Cong Bao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Now reading ./Encoded/{model_to_use}.npy...\")\n",
    "encoded = np.load(f'./Encoded/{model_to_use}.npy')\n",
    "config = AutoConfig.from_pretrained(MODEL_TO_USE)\n",
    "#last part of encoded is the embeddings at the input, so they're all the same: the input embedding for [CLS] at before it's fed into the network\n",
    "encoded = encoded[:, :-config.dim]\n",
    "encoded = encoded.astype(dtype=np.float32, copy=False)\n",
    "\n",
    "X_train, X_valid = train_test_split(encoded, test_size=0.15)\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices(X_train)\n",
    "dataset_valid = tf.data.Dataset.from_tensor_slices(X_valid)\n",
    "\n",
    "dataset_train = dataset_train.map(lambda x: (x,x))\n",
    "dataset_train = dataset_train.shuffle(10000)\n",
    "dataset_train = dataset_train.batch(batch_size)\n",
    "\n",
    "dataset_valid = dataset_valid.map(lambda x: (x,x))\n",
    "dataset_valid = dataset_valid.shuffle(10000)\n",
    "dataset_valid = dataset_valid.batch(batch_size)\n",
    "\n",
    "\n",
    "#From Aurelien Geron's Hands-on Machine Learning 2nd ed. https://github.com/ageron/handson-ml2/blob/master/17_autoencoders_and_gans.ipynb\n",
    "class DenseTranspose(keras.layers.Layer):\n",
    "    def __init__(self, dense, activation=None, **kwargs):\n",
    "        self.dense=dense\n",
    "        self.activation = keras.activations.get(activation)\n",
    "        super().__init__(**kwargs)\n",
    "    def build(self, batch_input_shape):\n",
    "        self.biases = self.add_weight(name=\"bias\", initializer=\"zeros\", shape=[self.dense.input_shape[-1]])\n",
    "        super().build(batch_input_shape)\n",
    "    def call(self, inputs):\n",
    "        z = tf.matmul(inputs, self.dense.weights[0], transpose_b=True)\n",
    "        return self.activation(z + self.biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "dense_1 = keras.layers.Dense(3072, activation=\"selu\")\n",
    "dense_2 = keras.layers.Dense(2048, activation=\"selu\")\n",
    "dense_3 = keras.layers.Dense(1024, activation=\"selu\")\n",
    "\n",
    "encoder = keras.models.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=d_train.element_spec[0].shape[1:]),\n",
    "    dense_1,\n",
    "    dense_2,\n",
    "    dense_3\n",
    "])\n",
    "\n",
    "tied_decoder = keras.models.Sequential([\n",
    "    DenseTranspose(dense_3, activation=\"selu\"),\n",
    "    DenseTranspose(dense_2, activation=\"selu\"),\n",
    "    DenseTranspose(dense_1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "tied_ae = keras.models.Sequential([encoder, tied_decoder])\n",
    "\n",
    "callback_list = [keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3)]\n",
    "\n",
    "tied_ae.compile(loss=\"mse\", optimizer=keras.optimizers.Adagrad(), metrics=[\"accuracy\"])\n",
    "\n",
    "hist = tied_ae.fit(d_train, epochs=10, validation_data=d_valid, callbacks=callback_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
