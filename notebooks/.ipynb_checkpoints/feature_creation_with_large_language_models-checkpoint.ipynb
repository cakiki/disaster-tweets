{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from transformers import TFAutoModel, TFBertModel\n",
    "from transformers import BertweetTokenizer, AutoTokenizer\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/external/kaggle/train.csv')\n",
    "test = pd.read_csv('../data/external/kaggle/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huggingface_embedder(df, col, tokenizer, model, batch_size=512):\n",
    "    to_tokenize = df[col].values\n",
    "    tokenized = tokenizer(list(to_tokenize), padding=True, truncation=True, return_tensors='tf', return_token_type_ids=False)\n",
    "    inputs = tokenized['input_ids']\n",
    "    masks = tokenized['attention_mask']\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((inputs, masks))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    embeddings = []\n",
    "    for input_tensor, attention_mask in dataset:\n",
    "        output = model([input_tensor, attention_mask])\n",
    "        embeddings.append(output.pooler_output.numpy())\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "def tf_hub_embedder(df, col, model, batch_size=512):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(df[col].values)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    embeddings = []\n",
    "    for batch in dataset:\n",
    "        embedding = model(batch)\n",
    "        embeddings.append(embedding.numpy())\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "def tf_hub_bert_embedder(df, col, model, preprocess, batch_size=512):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(df[col].values)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    embeddings = []\n",
    "    for batch in dataset:\n",
    "        inputs = preprocess(batch)\n",
    "        embedding = model(inputs)['pooled_output']\n",
    "        embeddings.append(embedding.numpy())\n",
    "    return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTweet Embeddings\n",
    "### BERTweet: A pre-trained language model for English Tweets (https://huggingface.co/vinai/bertweet-base)\n",
    "source https://arxiv.org/abs/2005.10200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "Some layers from the model checkpoint at vinai/bertweet-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at vinai/bertweet-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "bertweet_tokenizer = BertweetTokenizer.from_pretrained(\"vinai/bertweet-base\", normalization=True)\n",
    "bertweet_model = TFAutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "bertweet_model.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':grinning_face_with_big_eyes:',\n",
       " ':smiling_face_with_smiling_eyes:',\n",
       " ':pleading_face:',\n",
       " ':winking_face:',\n",
       " ':smiling_face_with_heart-eyes:',\n",
       " ':face_blowing_a_kiss:',\n",
       " ':smiling_face_with_3_hearts:',\n",
       " ':regional_indicator_symbol_letter_f:',\n",
       " ':regional_indicator_symbol_letter_r:']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('üòÉüòäü•∫üòâüòçüòòü•∞üá´üá∑')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.1 s, sys: 3.85 s, total: 20 s\n",
      "Wall time: 17.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_bertweet_embeddings = huggingface_embedder(train, 'text', bertweet_tokenizer, bertweet_model)\n",
    "test_bertweet_embeddings = huggingface_embedder(test, 'text', bertweet_tokenizer, bertweet_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Universal Sentence Encoder\n",
    "### Encoder of greater-than-word length text trained on a variety of data. (https://tfhub.dev/google/universal-sentence-encoder/4)\n",
    "Source: https://arxiv.org/abs/1803.11175"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.gstatic.com/aihub/tfhub/universal-sentence-encoder/example-similarity.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using /tmp/tfhub_modules to cache modules.\n",
      "INFO:absl:Downloading TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder/4'.\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 180.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 350.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 520.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 700.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 880.00MB\n",
      "INFO:absl:Downloaded https://tfhub.dev/google/universal-sentence-encoder/4, Total size: 987.47MB\n",
      "INFO:absl:Downloaded TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder/4'.\n"
     ]
    }
   ],
   "source": [
    "universal_sentence_encoder = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.1 s, sys: 338 ms, total: 2.44 s\n",
      "Wall time: 1.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_use4_embeddings = tf_hub_embedder(train, 'text', universal_sentence_encoder)\n",
    "test_use4_embeddings = tf_hub_embedder(test, 'text', universal_sentence_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/features/train_use4_embeddings.npy', 'wb') as f:\n",
    "    np.save(f, train_use4_embeddings)\n",
    "with open('../data/features/test_use4_embeddings.npy', 'wb') as f:\n",
    "    np.save(f, test_use4_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nnlm-en-dim128-with-normalization\n",
    "### Text embedding based on feed-forward Neural-Net Language Models with pre-built OOV trained on English Google News 200B corpus.\n",
    "\n",
    "\n",
    "### Maps from text to 128-dimensional embedding vectors. (https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2)\n",
    "Source: https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using /tmp/tfhub_modules to cache modules.\n",
      "INFO:absl:Downloading TF-Hub Module 'https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2'.\n",
      "INFO:absl:Downloading https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2: 180.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2: 360.00MB\n",
      "INFO:absl:Downloaded https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2, Total size: 483.55MB\n",
      "INFO:absl:Downloaded TF-Hub Module 'https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2'.\n"
     ]
    }
   ],
   "source": [
    "nnlm_en_128_norm = hub.load(\"https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 394 ms, sys: 62.4 ms, total: 457 ms\n",
      "Wall time: 342 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_nnlm_en_128_norm_embeddings = tf_hub_embedder(train, 'text', nnlm_en_128_norm)\n",
    "test_nnlm_en_128_norm_embeddings = tf_hub_embedder(test, 'text', nnlm_en_128_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/features/train_nnlm_en_128_norm_embeddings.npy', 'wb') as f:\n",
    "    np.save(f, train_nnlm_en_128_norm_embeddings)\n",
    "with open('../data/features/test_nnlm_en_128_norm_embeddings.npy', 'wb') as f:\n",
    "    np.save(f, test_nnlm_en_128_norm_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Large Cased with Whole Word Masking\n",
    "### Pooled output dense embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-large-cased-whole-word-masking were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-large-cased-whole-word-masking.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-large-cased-whole-word-masking\")\n",
    "bert_model = TFBertModel.from_pretrained(\"bert-large-cased-whole-word-masking\")\n",
    "bert_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min, sys: 36.4 s, total: 1min 37s\n",
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_bert_large_cased_whole_embeddings = huggingface_embedder(train, 'text', bert_tokenizer, bert_model)\n",
    "test_bert_large_cased_whole_embeddings = huggingface_embedder(test, 'text', bert_tokenizer, bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/features/train_bert_large_cased_whole_embeddings.npy', 'wb') as f:\n",
    "    np.save(f, train_bert_large_cased_whole_embeddings)\n",
    "with open('../data/features/test_bert_large_cased_whole_embeddings.npy', 'wb') as f:\n",
    "    np.save(f, test_bert_large_cased_whole_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tfhub BERT SST2 expert: \n",
    "### Pooled output dense embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using /tmp/tfhub_modules to cache modules.\n"
     ]
    }
   ],
   "source": [
    "preprocess_sst2 = hub.load('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')\n",
    "bert_sst2 = hub.load('https://tfhub.dev/google/experts/bert/wiki_books/sst2/2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 30s, sys: 2min 13s, total: 9min 44s\n",
      "Wall time: 3min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_bert_sst2_embeddings = tf_hub_bert_embedder(train, 'text', bert_sst2, preprocess_sst2)\n",
    "test_bert_sst2_embeddings = tf_hub_bert_embedder(test, 'text', bert_sst2, preprocess_sst2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 768)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bert_sst2_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3263, 768)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_bert_sst2_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/features/train_bert_sst2_embeddings.npy', 'wb') as f:\n",
    "    np.save(f, train_bert_sst2_embeddings)\n",
    "with open('../data/features/test_bert_sst2_embeddings.npy', 'wb') as f:\n",
    "    np.save(f, test_bert_sst2_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
