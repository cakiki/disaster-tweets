{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tempate for Project Classifiers\n",
    "This notebook can be used as template to roll your own classification models for the project. The template implements the classifier from the <a href=\"https://www.kaggle.com/philculliton/nlp-getting-started-tutorial\">Getting Started Tutorial</a> related to this Kaggle challenge. Each model must implement the <a href=\"https://scikit-learn.org/stable/developers/develop.html\">scikit-learn API</a>. The easiest way to do this is, is by inheriting from the base estimator and classifier mixin classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API requires us to implement two functions in our custom model, namely `fit` and `predict`. We will usually also implement an `__init__` and `score` function (the default score function is the mean accuracy, which only makes sense for balanced classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn import metrics, feature_extraction, linear_model\n",
    "\n",
    "class TemplateClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Setup model parameters and instance attributes\n",
    "        self.count_vectorizer = feature_extraction.text.CountVectorizer()\n",
    "        self.clf = linear_model.RidgeClassifier()\n",
    "\n",
    "    def score(self, X, y, sample_weight=None):\n",
    "        # We use the F1 score to have the same evaluation metric as in the challenge\n",
    "        return metrics.f1_score(y, self.predict(X), sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here comes the training method, it takes the feature matrix `X` in shape `(n_samples, n_features)` and the binary target vector `(n_samples,)`. The feature matrix has 3 columns (in order):\n",
    "\n",
    "* `keyword`: The keyword feature of the tweets (may have null entries)\n",
    "* `location`: The location faeture of the tweets (may have null entries)\n",
    "* `text`: The actual tweet (non-null)\n",
    "\n",
    "The target vector `y` consists of the two classes `[0, 1]`. Note that class `1` is slightly underrepresented (run notebook `exploratory_data_analysis` for first details on the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemplateClassifier(TemplateClassifier):\n",
    "    def fit(self, X, y):\n",
    "        # We implement model training by first applying all feature transformations\n",
    "        X_trans = self.count_vectorizer.fit_transform(X[:, 2])\n",
    "        # We then train the model with the vectorized feature matrix\n",
    "        self.clf.fit(X_trans, y)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `predict` method takes a feature matrix `X` in shape `(n_samples, n_features)` containing the same columns as described in the training function above (i.e. `keyword`, `location` and `text`). The function must return a vector of shape `(n_samples,)` containing the binary prediction `[0, 1]` for each sample in `X`. Some pitfalls are for example, that you don't apply the same feature transformation as in model training (forgotten, change of order, etc.). If you ahve a lot of feature transformations, consider using the pipeline model approach (see the notebook `template_model_pipeline` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemplateClassifier(TemplateClassifier):\n",
    "    def predict(self, X):\n",
    "        # Perform some checks\n",
    "        check_is_fitted(self.count_vectorizer)\n",
    "        check_is_fitted(self.clf)\n",
    "        # Dont forget to apply the same transformations that were used for training\n",
    "        X_trans = self.count_vectorizer.transform(X[:, 2])\n",
    "        # Compute and return the predictions\n",
    "        return self.clf.predict(X_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and Submission File\n",
    "Once we have the model defined, we run our shared evaluation pipeline. The model performance is measured using stratified cross-validation (i.e. the training data is repeatedly split into training and test set until every data point was once used as test set). This type of evaluation might take a while depending on how fast your model trains and compute predictions. When finished, a bunch of performance metrics for your model are printed:\n",
    "\n",
    "* <b>F1-Score</b>: The main metric which is used by the challenge to evaluate the model. The score combines recall and precision (see below for details) into a single score that takes class inbalance into account.\n",
    "* <b>Accuracy</b>: A standard score used by many classifiers, but prone to misinterpretation with inbalanced class distributions.\n",
    "* <b>Recall</b>: The ability of the model to detect tweets about real disasters (i.e. the probability that the model actually finds real disaster tweets).\n",
    "* <b>Precision</b>: The ability of the model to correctly classify tweets about real disaster (i.e. the probability that tweets classified as real disaster tweets by the model are actually real disaster tweets).\n",
    "\n",
    "The evaluation will also store the model and create a submission file for the challenge if the corresponding flags are set (all outsputs are stored in the `/models` directory). Stored files are labeled with a datetime stamp, followed by the model class name, the cross-calidation settings and the the F1 score it achieved in CV.\n",
    "E.g. the submission file `submission_2020-12-05_224810_TemplateClassifier_1x5cv_0.73.csv` was created on 5.12.2020 at 22:45:10 for a `TemplateClassifier` model achieving an F1-Score of `0.73` using 1 run of 5-Fold Cross-Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading training data from ../data/external/kaggle/train.csv...\n",
      "INFO:root:-> Number of samples: 7613\n",
      "INFO:root:-> Number of features: 3\n",
      "INFO:root:Evaluating model with 1 experiment(s) of 10-fold Cross Validation...\n",
      "INFO:root:Run 1/10 finished\n",
      "INFO:root:Run 2/10 finished\n",
      "INFO:root:Run 3/10 finished\n",
      "INFO:root:Run 4/10 finished\n",
      "INFO:root:Run 5/10 finished\n",
      "INFO:root:Run 6/10 finished\n",
      "INFO:root:Run 7/10 finished\n",
      "INFO:root:Run 8/10 finished\n",
      "INFO:root:Run 9/10 finished\n",
      "INFO:root:Run 10/10 finished\n",
      "INFO:root:---\n",
      "INFO:root:Expected submission results (F1-Score): around 0.74\n",
      "INFO:root:F1-Score: 1.00 (training); 0.74 (test)\n",
      "INFO:root:Accuracy: 99.57% (training); 78.81% (test)\n",
      "INFO:root:Recall: 99.36% (training); 69.67% (test)\n",
      "INFO:root:Precision: 99.64% (training); 78.59% (test)\n",
      "INFO:root:---\n",
      "INFO:root:Retraining model on the complete data set...\n",
      "INFO:root:-> F1-Score on complete training set: 0.99\n",
      "INFO:root:-> Stored model to ../models/model_2020-12-06_124915_TemplateClassifier_1x10cv_0.74.pck\n",
      "INFO:root:-> Stored submission file to ../models/submission_2020-12-06_124915_TemplateClassifier_1x10cv_0.74.csv\n",
      "INFO:root:Evaluation finished.\n"
     ]
    }
   ],
   "source": [
    "import evaluation\n",
    "\n",
    "model = TemplateClassifier()\n",
    "evaluation.evaluate(model, store_model=True, store_submission=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results can be used to assess on what to improve on the model. Some basic guidelines:\n",
    "\n",
    "* If there is a big discrepancy between training and test scores, your model might be too strong. Consider decreasing its power by tuning the parameters towards less complex models.\n",
    "* If traing and test scores are close and low, your model might be too weak. Consider increasing its power by tuning the parameters towards more complex models.\n",
    "* If there is a big discrepancy between recall and precision, you model might have issues with the class inbalance. Consider class balancing for preprocessing or weighting classes in model training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual submission result is `0.78057`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
