{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import Phrases\n",
    "from itertools import chain\n",
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/external/kaggle/train.csv')\n",
    "test = pd.read_csv('../data/external/kaggle/test.csv')\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview_corpus(corpus):\n",
    "    for i,doc in enumerate(corpus[:5],1):\n",
    "        print(f'Document {i}: {doc}')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1: ['our', 'deeds', 'are', 'the', 'reason', 'of', 'this', 'earthquake', 'may', 'allah', 'forgive', 'us', 'all']\n",
      "\n",
      "Document 2: ['forest', 'fire', 'near', 'la', 'ronge', 'sask', 'canada']\n",
      "\n",
      "Document 3: ['all', 'residents', 'asked', 'to', 'shelter', 'in', 'place', 'are', 'being', 'notified', 'by', 'officers', 'no', 'other', 'evacuation', 'or', 'shelter', 'in', 'place', 'orders', 'are', 'expected']\n",
      "\n",
      "Document 4: ['people', 'receive', 'wildfires', 'evacuation', 'orders', 'in', 'california']\n",
      "\n",
      "Document 5: ['just', 'got', 'sent', 'this', 'photo', 'from', 'ruby', 'alaska', 'as', 'smoke', 'from', 'wildfires', 'pours', 'into', 'school']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corpus = train['text'].values\n",
    "corpus = [doc.lower() for doc in corpus]\n",
    "corpus = [tokenizer.tokenize(doc) for doc in corpus]\n",
    "corpus = [[token for token in doc if (not token.isnumeric() and len(token) > 1)] for doc in corpus]\n",
    "preview_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1: ['our', 'deeds', 'are', 'the', 'reason', 'of', 'this', 'earthquake', 'may', 'allah', 'forgive', 'us', 'all']\n",
      "\n",
      "Document 2: ['forest', 'fire', 'near', 'la', 'ronge', 'sask', 'canada']\n",
      "\n",
      "Document 3: ['all', 'residents', 'asked', 'to', 'shelter', 'in', 'place', 'are', 'being', 'notified', 'by', 'officers', 'no', 'other', 'evacuation', 'or', 'shelter', 'in', 'place', 'orders', 'are', 'expected']\n",
      "\n",
      "Document 4: ['people', 'receive', 'wildfires', 'evacuation', 'orders', 'in', 'california']\n",
      "\n",
      "Document 5: ['just', 'got', 'sent', 'this', 'photo', 'from', 'ruby', 'alaska', 'as', 'smoke', 'from', 'wildfires', 'pours', 'into', 'school']\n",
      "\n",
      "CPU times: user 4.34 s, sys: 2.96 ms, total: 4.35 s\n",
      "Wall time: 4.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "bigram = Phrases(corpus, min_count=20)\n",
    "trigram = Phrases(bigram[corpus], min_count=10)\n",
    "fourgram = Phrases(trigram[corpus], min_count=10)\n",
    "for doc in corpus:\n",
    "    bigrams = [b for b in bigram[doc] if b.count('_') == 1]\n",
    "    trigrams = [t for t in trigram[bigram[doc]] if t.count('_') == 2]\n",
    "    fourgrams = [f for f in fourgram[trigram[bigram[doc]]] if f.count('_') == 3]\n",
    "    doc.extend(list(chain(*[bigrams, trigrams, fourgrams])))\n",
    "preview_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(corpus)\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1: [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1)]\n",
      "\n",
      "Document 2: [(10, 1), (11, 1), (12, 1), (13, 1)]\n",
      "\n",
      "Document 3: [(0, 1), (1, 2), (14, 1), (15, 1), (16, 1), (17, 2), (18, 1), (19, 1), (20, 1), (21, 2), (22, 1)]\n",
      "\n",
      "Document 4: [(16, 1), (17, 1), (23, 1), (24, 1)]\n",
      "\n",
      "Document 5: [(8, 1), (25, 1), (26, 2), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preview_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 3677\n",
      "Number of documents: 7613\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.81 s, sys: 0 ns, total: 2.81 s\n",
      "Wall time: 2.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_topics = 5\n",
    "chunksize = 1000\n",
    "passes = 1\n",
    "iterations = 400\n",
    "eval_every = None\n",
    "\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average topic coherence: -4.5080.\n",
      "[([(0.053781196, 'the'),\n",
      "   (0.039594475, 'you'),\n",
      "   (0.033763558, 'to'),\n",
      "   (0.0305492, 'and'),\n",
      "   (0.02576075, 'that'),\n",
      "   (0.023221346, 'my'),\n",
      "   (0.02148747, 'of'),\n",
      "   (0.01904881, 'it'),\n",
      "   (0.0189762, 'be'),\n",
      "   (0.01846861, 'have'),\n",
      "   (0.018052092, 'is'),\n",
      "   (0.01219985, 'if'),\n",
      "   (0.011694165, 'me'),\n",
      "   (0.011619976, 'but'),\n",
      "   (0.011431983, 'will'),\n",
      "   (0.0114151845, 'so'),\n",
      "   (0.011187858, 'with'),\n",
      "   (0.010726236, 'for'),\n",
      "   (0.010488652, 'not'),\n",
      "   (0.0103229275, 'your')],\n",
      "  -2.4843459018751357),\n",
      " ([(0.05674868, 'the'),\n",
      "   (0.03135121, 'in'),\n",
      "   (0.03087665, 'to'),\n",
      "   (0.023665862, 'of'),\n",
      "   (0.022444809, 'and'),\n",
      "   (0.017965, 'it'),\n",
      "   (0.01456986, 'wreck'),\n",
      "   (0.013613525, 'for'),\n",
      "   (0.012887438, 'this'),\n",
      "   (0.012196167, 'was'),\n",
      "   (0.011660811, 'like'),\n",
      "   (0.011472275, 'wounded'),\n",
      "   (0.010971312, 'http'),\n",
      "   (0.010756469, 'my'),\n",
      "   (0.009790454, 'http_co'),\n",
      "   (0.0095378095, 'fires'),\n",
      "   (0.009392059, 'disaster'),\n",
      "   (0.009186263, 'on'),\n",
      "   (0.008290558, 'is'),\n",
      "   (0.008219861, 'obama')],\n",
      "  -4.065847253028392),\n",
      " ([(0.16726093, 'http'),\n",
      "   (0.16065522, 'http_co'),\n",
      "   (0.022569144, 'in'),\n",
      "   (0.017857933, 'for'),\n",
      "   (0.016228529, 'at'),\n",
      "   (0.014973399, 'รป_'),\n",
      "   (0.014875501, 'suicide'),\n",
      "   (0.012991854, 'typhoon'),\n",
      "   (0.0117748305, 'video'),\n",
      "   (0.011047146, 'pm'),\n",
      "   (0.010925819, 'after'),\n",
      "   (0.010722842, 'news'),\n",
      "   (0.010606187, 'of'),\n",
      "   (0.0092874905, 'from'),\n",
      "   (0.009154747, 'killed'),\n",
      "   (0.008028588, 'to'),\n",
      "   (0.00801814, 'california'),\n",
      "   (0.007893455, 'via'),\n",
      "   (0.007299341, 'by'),\n",
      "   (0.0067848274, 'the')],\n",
      "  -4.258012363915584),\n",
      " ([(0.07771776, 'http'),\n",
      "   (0.07583963, 'http_co'),\n",
      "   (0.061312187, 'the'),\n",
      "   (0.04293029, 'of'),\n",
      "   (0.03408281, 'in'),\n",
      "   (0.025765538, 'to'),\n",
      "   (0.017120747, 'is'),\n",
      "   (0.013563096, 'and'),\n",
      "   (0.01344549, 'by'),\n",
      "   (0.01283441, 'on'),\n",
      "   (0.010719466, 'for'),\n",
      "   (0.010601665, 'storm'),\n",
      "   (0.008707125, 'police'),\n",
      "   (0.007626056, 'wreckage'),\n",
      "   (0.007595755, 'weapon'),\n",
      "   (0.007339641, 'weapons'),\n",
      "   (0.0068878145, 'wild'),\n",
      "   (0.006257698, 'nuclear'),\n",
      "   (0.00592855, 'from'),\n",
      "   (0.0057699406, 'more')],\n",
      "  -5.454354208087831),\n",
      " ([(0.030783975, 'https_co'),\n",
      "   (0.03075798, 'https'),\n",
      "   (0.025023295, 'in'),\n",
      "   (0.019457148, 'and'),\n",
      "   (0.01618583, 'mh370'),\n",
      "   (0.014876427, 'on'),\n",
      "   (0.014786358, 'this'),\n",
      "   (0.013408775, 'is'),\n",
      "   (0.013056829, 'confirmed'),\n",
      "   (0.012075799, 'to'),\n",
      "   (0.011797591, 'up'),\n",
      "   (0.011357827, 'malaysia'),\n",
      "   (0.011138746, 'families'),\n",
      "   (0.010832197, 'you'),\n",
      "   (0.010779997, 'from'),\n",
      "   (0.01067102, 'dead'),\n",
      "   (0.010417969, 'those'),\n",
      "   (0.01021363, 'with'),\n",
      "   (0.009764656, 'from_mh370'),\n",
      "   (0.009421361, 'they')],\n",
      "  -6.277509828222212)]\n"
     ]
    }
   ],
   "source": [
    "top_topics = model.top_topics(corpus) #, num_words=20)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.44 s, sys: 60.3 ms, total: 2.5 s\n",
      "Wall time: 2.42 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.013006</td>\n",
       "      <td>0.010942</td>\n",
       "      <td>0.012808</td>\n",
       "      <td>0.942703</td>\n",
       "      <td>0.020542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.029287</td>\n",
       "      <td>0.024716</td>\n",
       "      <td>0.028954</td>\n",
       "      <td>0.350792</td>\n",
       "      <td>0.566252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010233</td>\n",
       "      <td>0.962967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.029348</td>\n",
       "      <td>0.024729</td>\n",
       "      <td>0.317562</td>\n",
       "      <td>0.031643</td>\n",
       "      <td>0.596717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.197301</td>\n",
       "      <td>0.292873</td>\n",
       "      <td>0.012839</td>\n",
       "      <td>0.475931</td>\n",
       "      <td>0.021056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>0.015923</td>\n",
       "      <td>0.013436</td>\n",
       "      <td>0.780345</td>\n",
       "      <td>0.017090</td>\n",
       "      <td>0.173208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.633804</td>\n",
       "      <td>0.345121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>0.029283</td>\n",
       "      <td>0.024716</td>\n",
       "      <td>0.029074</td>\n",
       "      <td>0.031476</td>\n",
       "      <td>0.885451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>0.012984</td>\n",
       "      <td>0.938424</td>\n",
       "      <td>0.012857</td>\n",
       "      <td>0.013970</td>\n",
       "      <td>0.021765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.712177</td>\n",
       "      <td>0.068196</td>\n",
       "      <td>0.206615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows ร 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4\n",
       "0     0.013006  0.010942  0.012808  0.942703  0.020542\n",
       "1     0.029287  0.024716  0.028954  0.350792  0.566252\n",
       "2     0.000000  0.000000  0.000000  0.010233  0.962967\n",
       "3     0.029348  0.024729  0.317562  0.031643  0.596717\n",
       "4     0.197301  0.292873  0.012839  0.475931  0.021056\n",
       "...        ...       ...       ...       ...       ...\n",
       "7608  0.015923  0.013436  0.780345  0.017090  0.173208\n",
       "7609  0.000000  0.000000  0.000000  0.633804  0.345121\n",
       "7610  0.029283  0.024716  0.029074  0.031476  0.885451\n",
       "7611  0.012984  0.938424  0.012857  0.013970  0.021765\n",
       "7612  0.000000  0.000000  0.712177  0.068196  0.206615\n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "topics = [model.get_document_topics(doc) for doc in corpus]\n",
    "topics = [dict(topic_dist) for topic_dist in topics]\n",
    "topics = pd.DataFrame(topics).fillna(0)\n",
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
