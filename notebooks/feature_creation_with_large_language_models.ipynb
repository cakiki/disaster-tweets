{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 20.3.3 is available.\r\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install emoji pandas transformers[sentencepiece] tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from transformers import TFAutoModel\n",
    "from transformers import BertweetTokenizer\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTweet Embeddings\n",
    "### BERTweet: A pre-trained language model for English Tweets\n",
    "https://arxiv.org/abs/2005.10200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "Some layers from the model checkpoint at vinai/bertweet-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at vinai/bertweet-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "bertweet_tokenizer = BertweetTokenizer.from_pretrained(\"vinai/bertweet-base\", normalization=True)\n",
    "bertweet_model = TFAutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "bertweet_model.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/external/kaggle/train.csv')\n",
    "test = pd.read_csv('../data/external/kaggle/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':grinning_face_with_big_eyes:',\n",
       " ':smiling_face_with_smiling_eyes:',\n",
       " ':pleading_face:',\n",
       " ':winking_face:',\n",
       " ':smiling_face_with_heart-eyes:',\n",
       " ':face_blowing_a_kiss:',\n",
       " ':smiling_face_with_3_hearts:',\n",
       " ':regional_indicator_symbol_letter_f:',\n",
       " ':regional_indicator_symbol_letter_r:']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('üòÉüòäü•∫üòâüòçüòòü•∞üá´üá∑')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huggingface_embedder(df, col, tokenizer, model, batch_size=512):\n",
    "    to_tokenize = df[col].values\n",
    "    tokenized = tokenizer(list(to_tokenize), padding=True, truncation=True, return_tensors='tf', return_token_type_ids=False)\n",
    "    inputs = tokenized['input_ids']\n",
    "    masks = tokenized['attention_mask']\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((inputs, masks))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    embeddings = []\n",
    "    for input_tensor, attention_mask in dataset:\n",
    "        output = model([input_tensor, attention_mask])\n",
    "        embeddings.append(output.pooler_output.numpy())\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "def tf_hub_embedder(df, col, model, batch_size=512):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(df[col].values)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    embeddings = []\n",
    "    for batch in dataset:\n",
    "        embedding = model(batch)\n",
    "        embeddings.append(embedding.numpy())\n",
    "    return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.1 s, sys: 3.85 s, total: 20 s\n",
      "Wall time: 17.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_bertweet_embeddings = huggingface_embedder(train, 'text', bertweet_tokenizer, bertweet_model)\n",
    "test_bertweet_embeddings = huggingface_embedder(test, 'text', bertweet_tokenizer, bertweet_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Universal Sentence Encoder\n",
    "### Encoder of greater-than-word length text trained on a variety of data. (https://tfhub.dev/google/universal-sentence-encoder/4)\n",
    "Source: https://arxiv.org/abs/1803.11175"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.gstatic.com/aihub/tfhub/universal-sentence-encoder/example-similarity.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using /tmp/tfhub_modules to cache modules.\n",
      "INFO:absl:Downloading TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder/4'.\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 180.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 350.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 520.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 700.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 880.00MB\n",
      "INFO:absl:Downloaded https://tfhub.dev/google/universal-sentence-encoder/4, Total size: 987.47MB\n",
      "INFO:absl:Downloaded TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder/4'.\n"
     ]
    }
   ],
   "source": [
    "universal_sentence_encoder = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.1 s, sys: 338 ms, total: 2.44 s\n",
      "Wall time: 1.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_use4_embeddings = tf_hub_embedder(train, 'text', universal_sentence_encoder)\n",
    "test_use4_embeddings = tf_hub_embedder(test, 'text', universal_sentence_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/features/train_use4_embeddings.npy', 'wb') as f:\n",
    "    np.save(f, train_use4_embeddings)\n",
    "with open('../data/features/test_use4_embeddings.npy', 'wb') as f:\n",
    "    np.save(f, test_use4_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nnlm-en-dim128-with-normalization\n",
    "### Text embedding based on feed-forward Neural-Net Language Models with pre-built OOV trained on English Google News 200B corpus.\n",
    "\n",
    "\n",
    "### Maps from text to 128-dimensional embedding vectors. (https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2)\n",
    "Source: https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using /tmp/tfhub_modules to cache modules.\n",
      "INFO:absl:Downloading TF-Hub Module 'https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2'.\n",
      "INFO:absl:Downloading https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2: 180.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2: 360.00MB\n",
      "INFO:absl:Downloaded https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2, Total size: 483.55MB\n",
      "INFO:absl:Downloaded TF-Hub Module 'https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2'.\n"
     ]
    }
   ],
   "source": [
    "nnlm_en_128_norm = hub.load(\"https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 394 ms, sys: 62.4 ms, total: 457 ms\n",
      "Wall time: 342 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_nnlm_en_128_norm_embeddings = tf_hub_embedder(train, 'text', nnlm_en_128_norm)\n",
    "test_nnlm_en_128_norm_embeddings = tf_hub_embedder(test, 'text', nnlm_en_128_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/features/train_nnlm_en_128_norm_embeddings.npy', 'wb') as f:\n",
    "    np.save(f, train_nnlm_en_128_norm_embeddings)\n",
    "with open('../data/features/test_nnlm_en_128_norm_embeddings.npy', 'wb') as f:\n",
    "    np.save(f, test_nnlm_en_128_norm_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
