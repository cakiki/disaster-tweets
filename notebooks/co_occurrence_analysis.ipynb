{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from ipywidgets import Output\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import Phrases\n",
    "from itertools import chain\n",
    "from gensim.corpora import Dictionary\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import chain\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 179 stopwords from NLTK\n",
      "Loaded 326 stopwords from SPACY\n",
      "Loaded 318 stopwords from SKLEARN\n",
      "----------------------------------\n",
      "409 combined stopwords\n",
      "CPU times: user 204 ms, sys: 20.8 ms, total: 224 ms\n",
      "Wall time: 601 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = pd.read_csv('../data/external/kaggle/train.csv')\n",
    "test = pd.read_csv('../data/external/kaggle/test.csv')\n",
    "y = pd.read_csv('../data/external/kaggle/train.csv')['target'].values\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as SKLEARN_STOPWORDS\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as SPACY_STOPWORDS\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "SKLEARN_STOPWORDS = set(SKLEARN_STOPWORDS)\n",
    "out = Output()\n",
    "with out:\n",
    "    nltk.download('stopwords')\n",
    "NLTK_STOPWORDS = set(stopwords.words('english'))\n",
    "print(f'Loaded {len(NLTK_STOPWORDS)} stopwords from NLTK')\n",
    "print(f'Loaded {len(SPACY_STOPWORDS)} stopwords from SPACY')\n",
    "print(f'Loaded {len(SKLEARN_STOPWORDS)} stopwords from SKLEARN')\n",
    "stop_words = list(set.union(*[SKLEARN_STOPWORDS, SPACY_STOPWORDS, NLTK_STOPWORDS]))\n",
    "print('----------------------------------')\n",
    "print(f'{len(stop_words)} combined stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.06 s, sys: 2.13 ms, total: 3.06 s\n",
      "Wall time: 3.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corpus = train['text'].values\n",
    "corpus = [doc.lower() for doc in corpus]\n",
    "corpus = [tokenizer.tokenize(doc) for doc in corpus]\n",
    "corpus = [[token for token in doc if (not token.isnumeric() and len(token) > 1)] for doc in corpus]\n",
    "corpus= [[token for token in doc if (not token in stop_words)] for doc in corpus]\n",
    "bigram = Phrases(corpus, min_count=1)\n",
    "trigram = Phrases(bigram[corpus], min_count=1)\n",
    "fourgram = Phrases(trigram[corpus], min_count=1)\n",
    "bigrams = []\n",
    "trigrams = []\n",
    "fourgrams = []\n",
    "for doc in corpus:\n",
    "    b = [b for b in bigram[doc] if b.count('_') == 1]\n",
    "    t = [t for t in trigram[bigram[doc]] if t.count('_') == 2]\n",
    "    f = [f for f in fourgram[trigram[bigram[doc]]] if f.count('_') == 3]\n",
    "    bigrams.extend(b)\n",
    "    trigrams.extend(t)\n",
    "    fourgrams.extend(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('รป_', 77),\n",
       " ('suicide_bomber', 49),\n",
       " ('burning_buildings', 48),\n",
       " ('liked_youtube', 42),\n",
       " ('heat_wave', 31),\n",
       " ('california_wildfire', 31),\n",
       " ('natural_disaster', 30),\n",
       " ('prebreak_best', 30),\n",
       " ('mass_murder', 30),\n",
       " ('looks_like', 28)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(bigrams).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('รป__http', 150),\n",
       " ('liked_youtube_video', 35),\n",
       " ('รป__https', 33),\n",
       " ('black_hat_http', 17),\n",
       " ('pamela_geller_http', 15),\n",
       " ('ambulance_helicopter_crash', 14),\n",
       " ('bigger_projected_http', 14),\n",
       " ('read_ebay_http', 12),\n",
       " ('subreddits_banned_quarantined', 11),\n",
       " ('hiroshima_atomic_bombing', 10)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(trigrams).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('latest_homes_razed_northern', 27),\n",
       " ('families_sue_legionnaires_families', 25),\n",
       " ('watch_airport_swallowed_sandstorm', 24),\n",
       " ('wreckage_conclusively_confirmed_mh370', 24),\n",
       " ('affected_fatal_outbreak_legionnaires', 22),\n",
       " ('suicide_bomber_detonated_bomb', 22),\n",
       " ('ignition_knock_detonation_sensor', 20),\n",
       " ('obama_declares_disaster_typhoon', 20),\n",
       " ('malaysia_pm_investigators_families', 20),\n",
       " ('video_picking_bodies_water', 18)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(fourgrams).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>suicide</td>\n",
       "      <td>bomber</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>burning</td>\n",
       "      <td>buildings</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>liked</td>\n",
       "      <td>youtube</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>heat</td>\n",
       "      <td>wave</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>california</td>\n",
       "      <td>wildfire</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3476</th>\n",
       "      <td>mount</td>\n",
       "      <td>wario</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3477</th>\n",
       "      <td>jt</td>\n",
       "      <td>ruff23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3478</th>\n",
       "      <td>thunderstorm</td>\n",
       "      <td>warnings</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3479</th>\n",
       "      <td>parts</td>\n",
       "      <td>ar</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3480</th>\n",
       "      <td>nc</td>\n",
       "      <td>ok</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3481 rows ร 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 a          b  weight\n",
       "0          suicide     bomber      49\n",
       "1          burning  buildings      48\n",
       "2            liked    youtube      42\n",
       "3             heat       wave      31\n",
       "4       california   wildfire      31\n",
       "...            ...        ...     ...\n",
       "3476         mount      wario       1\n",
       "3477            jt     ruff23       1\n",
       "3478  thunderstorm   warnings       1\n",
       "3479         parts         ar       1\n",
       "3480            nc         ok       1\n",
       "\n",
       "[3481 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(Counter(bigrams).most_common()).rename(columns={0:'bigrams', 1:'weight'})\n",
    "df[['a', 'b']] = df['bigrams'].str.split(pat='_', expand=True)\n",
    "df = df[['a','b','weight']]\n",
    "df = df.replace('', np.nan).dropna().reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_pandas_edgelist(df, 'a', 'b', 'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_graphml(G, '../data/processed/bigrams.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
